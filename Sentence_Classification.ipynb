{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "import re\n",
    "import nltk.data\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import LineTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a function to extract and tokenize individual sentences from the corpus\n",
    "\n",
    "def tokenize_sentences(Directory):\n",
    "    label = {}\n",
    "    sentence = {}\n",
    "    normalized_sentence = {}\n",
    "    features = {}\n",
    "    global train\n",
    "    k = 0\n",
    "    for filename in listdir(Directory):          #Reading each file from the corpus\n",
    "        if filename.endswith(\".txt\"):\n",
    "            f = open(filename,'rb')\n",
    "            data = f.read()\n",
    "            i = 0;\n",
    "            while data is not None:\n",
    "                train = LineTokenizer(blanklines = 'discard').tokenize(data)  #tokenizing the text files\n",
    "                try:\n",
    "                    # preparing the training dataset with the extracted sentences and the corresponding labels\n",
    "                    if any(c in train[i][:4] for c in ('stopwords','own','base','contrast','aim')):\n",
    "                        label[k] = train[i][:4]\n",
    "                        sentence[k] = train[i][5:][0:]\n",
    "                        # removing the stopwords from the sentences using the stopwords list of nltk library\n",
    "                        normalized_sentence[k] = ' '.join([w for w in re.split('\\W',sentence[k]) if w.lower() not in stopwords.words('english') and w.lower() != ''])\n",
    "                except IndexError:\n",
    "                    break \n",
    "                i += 1\n",
    "                k += 1\n",
    "    \n",
    "    return label, normalized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to store the tokenized training data in a CSV file\n",
    "\n",
    "def export_training_data_to_csv(filename):\n",
    "    training_set = {}\n",
    "    with open(filename,'wb') as csvfile:\n",
    "            fieldnames = ['normalized_sentence','label']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "            for i in range(1,len(label),1):\n",
    "                try:\n",
    "                    writer.writerow({'normalized_sentence': normalized_sentence[i],'label': label[i]})\n",
    "                except IndexError:\n",
    "                    continue   \n",
    "                except KeyError:\n",
    "                    continue\n",
    "            sys.stdout.flush()\n",
    "            os.fsync(csvfile.fileno())\n",
    "            csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the Directory for the corpus\n",
    "Directory = \"/home/mukut/Downloads/SentenceCorpus/SentenceCorpus/labeled_articles/training_dataset\"\n",
    "# Setting the tokenizer - Pretrained tokenizer model\n",
    "import nltk\n",
    "import os\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "os.chdir(r'/home/mukut/Downloads/SentenceCorpus/SentenceCorpus/labeled_articles/training_dataset')\n",
    "# A call to the pre-defined function to tokenize the sentences\n",
    "label, normalized_sentence = tokenize_sentences(Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function call to store the tokenized data in a CSV file named, training_data.csv\n",
    "export_training_data_to_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the Naive Bayes Classifier\n",
    "\n",
    "with open('training_data.csv','rb') as csvfile:\n",
    "    C = NaiveBayesClassifier(csvfile,format = \"csv\")\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting the directory for the test dataset\n",
    "Directory = \"/home/mukut/Downloads/SentenceCorpus/SentenceCorpus/labeled_articles/test_dataset/\"\n",
    "os.chdir(r'/home/mukut/Downloads/SentenceCorpus/SentenceCorpus/labeled_articles/test_dataset/')\n",
    "# Function call to tokenize the test dataset \n",
    "label, normalized_sentence = tokenize_sentences(Directory)\n",
    "# Storing the tokenized test dataset in a CSV file test_data.csv\n",
    "export_training_data_to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing the accuracy of the Classifier using 30% of the labeled corpus as the test data\n",
    "\n",
    "accuracy = 0\n",
    "line_number = 0\n",
    "with open('test_data.csv','rb') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        reader = list(reader)\n",
    "        for filename in listdir(Directory):\n",
    "            if (filename.endswith(\".txt\")):\n",
    "                f = open(filename, 'rb')\n",
    "                data = f.readlines()\n",
    "                for sentence in data:\n",
    "                    # removing the stopwords from the sentences using the stopwords list of nltk library\n",
    "                    normalized_sentence = ' '.join([w for w in re.split('\\W',sentence) if w.lower() not in stopwords.words('english')])\n",
    "                    # Classifying the individual sentences\n",
    "                    blob = TextBlob(normalized_sentence, classifier = C)\n",
    "                    if line_number < len(reader):\n",
    "                    # Comparing the classified label with the pre-defined label in the test dataset\n",
    "                        if reader[line_number][1] == blob.classify():\n",
    "                        # Incrementing the accuracy for each match\n",
    "                            accuracy = accuracy + 1                    \n",
    "                        line_number += 1\n",
    "        csvfile.close()\n",
    "#print((accuracy/len(reader))* 100, '.2f') # Printing the accuracy of the Classifier\n",
    "# I get accuracy around 61%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifying unknown sentences and writing the results to a CSV file\n",
    "\n",
    "Directory = '/home/mukut/Downloads/SentenceCorpus/SentenceCorpus/unlabeled_articles'\n",
    "os.chdir(r'/home/mukut/Downloads/SentenceCorpus/SentenceCorpus/unlabeled_articles')\n",
    "with open('classified_sentences.csv','wb') as csvfile:\n",
    "    fieldnames = ['Sentence', 'Label']\n",
    "    writer = csv.DictWriter(csvfile,fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "    for filename in listdir(Directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            f = open(filename, 'rb')\n",
    "            data = f.readlines()\n",
    "            for sentence in data:\n",
    "                if not(sentence.startswith('###')):\n",
    "                    # removing the stopwords from the sentences using the stopwords list of nltk library\n",
    "                    normalized_sentence = ' '.join([w for w in re.split('\\W',sentence) if w.lower() not in stopwords.words('english')])\n",
    "                    # Classifying the individual sentences\n",
    "                    blob = TextBlob(normalized_sentence, classifier = C)\n",
    "                    # Writing the sentences along with their classified label to the CSV file\n",
    "                    writer.writerow({'Sentence': sentence, 'Label': blob.classify()})\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
